{"metadata": {"language_info": {"file_extension": ".scala", "mimetype": "text/x-scala", "version": "2.11.8", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}, "kernelspec": {"display_name": "Scala 2.11 with Spark 2.0", "language": "scala", "name": "scala-spark20"}}, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Get Hadoop Access"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "\nimport org.apache.spark.sql.SparkSession\n\n// @hidden_cell\n// This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n// You might want to remove those credentials before you share your notebook.\ndef setHadoopConfig18a70142643c45b7981f698a88e430e2(name: String) = {\n    // This function sets the Hadoop configuration so it is possible to\n    // access data from Bluemix Object Storage using Spark\n\n    val prefix = \"fs.swift.service.\" + name\n    sc.hadoopConfiguration.set(prefix + \".auth.url\", \"https://identity.open.softlayer.com\" + \"/v3/auth/tokens\")\n    sc.hadoopConfiguration.set(prefix + \".auth.endpoint.prefix\",\"endpoints\")\n    sc.hadoopConfiguration.set(prefix + \".tenant\", \"d31104c25a814c6c91023cd95c34cfc5\")\n    sc.hadoopConfiguration.set(prefix + \".username\", \"48c530d8f60f48688f3a81a28d73b2ae\")\n    sc.hadoopConfiguration.set(prefix + \".password\", \"v2v36a5U?ugFD]QD\")\n    sc.hadoopConfiguration.setInt(prefix + \".http.port\", 8080)\n    sc.hadoopConfiguration.set(prefix + \".region\", \"dallas\")\n    sc.hadoopConfiguration.setBoolean(prefix + \".public\", false)\n}\n\n// you can choose any name\nval name = \"keystone\"\nsetHadoopConfig18a70142643c45b7981f698a88e430e2(name)\n\nval spark = SparkSession.\n    builder().\n    getOrCreate()\n\nval df_churn = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"true\").\n    option(\"inferSchema\", \"true\").\n    load(\"swift://DubaiH2OMeetup.\" + name + \"/churn.all.csv\")\n", "execution_count": 23, "outputs": [{"output_type": "stream", "text": "1\n", "name": "stdout"}]}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "df_churn.select(\"state\", \"phone_number\", \"international_plan\", \"voice_mail_plan\").show(10)", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "+-----+------------+------------------+---------------+\n|state|phone_number|international_plan|voice_mail_plan|\n+-----+------------+------------------+---------------+\n|   KS|    382-4657|                no|            yes|\n|   OH|    371-7191|                no|            yes|\n|   NJ|    358-1921|                no|             no|\n|   OH|    375-9999|               yes|             no|\n|   OK|    330-6626|               yes|             no|\n|   AL|    391-8027|               yes|             no|\n|   MA|    355-9993|                no|            yes|\n|   MO|    329-9001|               yes|             no|\n|   LA|    335-4719|                no|             no|\n|   WV|    330-8173|               yes|            yes|\n+-----+------------+------------------+---------------+\nonly showing top 10 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Split into train & test datasets"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "val Array(train_churn, test_churn) = df_churn.randomSplit(Array(.7, .3), seed=1)", "execution_count": 1, "outputs": []}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "train_churn.count", "execution_count": 2, "outputs": [{"output_type": "execute_result", "execution_count": 2, "data": {"text/plain": "3490"}, "metadata": {}}]}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "test_churn.count", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "1510"}, "metadata": {}}]}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "df_churn.count", "execution_count": 7, "outputs": [{"output_type": "execute_result", "execution_count": 7, "data": {"text/plain": "5000"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Feature Extraction"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "import org.apache.spark.ml.feature.{ StringIndexer, IndexToString }\n\nval churn_indexer = new StringIndexer().\n                        setInputCol(\"churned\").\n                        setOutputCol(\"churn_label\").\n                        fit(df_churn)\n\nval churn_originalLabel_converter = new IndexToString().\n                                        setInputCol(\"prediction\").\n                                        setOutputCol(\"predictedLabel\").\n                                        setLabels(churn_indexer.labels)\n\nval international_plan_indexer = new StringIndexer().\n                                    setInputCol(\"international_plan\").\n                                    setOutputCol(\"international_plan_label\")\n\nval voice_mail_plan_indexer = new StringIndexer().\n                                    setInputCol(\"voice_mail_plan\").\n                                    setOutputCol(\"voice_mail_plan_label\")", "execution_count": 9, "outputs": []}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "val excluded_cols = List(\"state\", \"area_code\", \"phone_number\", \"international_plan\", \"voice_mail_plan\", \"churned\")\nval num_features = df_churn.columns.filterNot(col => excluded_cols.contains(col))\nnum_features", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "Array(account_length, number_vmail_messages, total_day_minutes, total_day_calls, total_day_charge, total_eve_minutes, total_eve_calls, total_eve_charge, total_night_minutes, total_night_calls, total_night_charge, total_intl_minutes, total_intl_calls, total_intl_charge, number_customer_service_calls)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Build the Pipeline"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "import org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.{ RandomForestClassifier }\nimport org.apache.spark.mllib.evaluation.{ BinaryClassificationMetrics, MulticlassMetrics }\nimport org.apache.spark.ml.evaluation.{ BinaryClassificationEvaluator }\nimport org.apache.spark.ml.linalg.{ DenseVector => MLDenseVector }\n\n\nval assembler = new VectorAssembler().\n                    setInputCols(num_features ++ Array(\"international_plan_label\", \"voice_mail_plan_label\")).\n                    setOutputCol(\"features\")\n\n\nval model = new RandomForestClassifier().\n                    setLabelCol(\"churn_label\").\n                    setFeaturesCol(\"features\")\n\nval pipeline = new Pipeline().setStages(Array(\n                    churn_indexer, \n                    international_plan_indexer, \n                    voice_mail_plan_indexer, \n                    assembler, \n                    model,\n                    churn_originalLabel_converter))", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Train the model on training dataset"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "val learned_model = pipeline.fit(train_churn)", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Perf. on training dataset (AUC)"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "val evaluator = new BinaryClassificationEvaluator().\n                        setLabelCol(\"churn_label\").\n                        setMetricName(\"areaUnderROC\")", "execution_count": 13, "outputs": []}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "val churn_predictions_train = learned_model.transform(train_churn)\nevaluator.evaluate(churn_predictions_train)", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "0.933808047398309"}, "metadata": {}}]}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "churn_predictions_train.select(\"phone_number\", \"churned\", \"churn_label\", \"prediction\", \"predictedLabel\", \"probability\").show()", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "+------------+-------+-----------+----------+--------------+--------------------+\n|phone_number|churned|churn_label|prediction|predictedLabel|         probability|\n+------------+-------+-----------+----------+--------------+--------------------+\n|    373-1028| False.|        0.0|       0.0|        False.|[0.96053113136784...|\n|    341-9764| False.|        0.0|       0.0|        False.|[0.95039189293736...|\n|    399-1526|  True.|        1.0|       0.0|        False.|[0.53300650560664...|\n|    378-7733| False.|        0.0|       0.0|        False.|[0.95410079297021...|\n|    389-7073| False.|        0.0|       0.0|        False.|[0.95740684022308...|\n|    362-8331| False.|        0.0|       0.0|        False.|[0.95845555551214...|\n|    352-9130| False.|        0.0|       0.0|        False.|[0.88622877332364...|\n|    375-5562| False.|        0.0|       0.0|        False.|[0.96396555229922...|\n|    356-5244| False.|        0.0|       0.0|        False.|[0.96030880317658...|\n|    365-6756| False.|        0.0|       0.0|        False.|[0.95010890739385...|\n|    364-1134| False.|        0.0|       0.0|        False.|[0.94317572125871...|\n|    416-1845| False.|        0.0|       0.0|        False.|[0.96119653177246...|\n|    366-5241| False.|        0.0|       0.0|        False.|[0.85863759638723...|\n|    346-8863| False.|        0.0|       0.0|        False.|[0.83567684600014...|\n|    332-2275| False.|        0.0|       0.0|        False.|[0.93947046599816...|\n|    366-9781| False.|        0.0|       0.0|        False.|[0.96435582894731...|\n|    336-5406| False.|        0.0|       0.0|        False.|[0.90142761500738...|\n|    418-9385| False.|        0.0|       0.0|        False.|[0.95495160129215...|\n|    386-3218| False.|        0.0|       0.0|        False.|[0.94371493694499...|\n|    395-8865| False.|        0.0|       0.0|        False.|[0.95439223283390...|\n+------------+-------+-----------+----------+--------------+--------------------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Perf. on test dataset (AUC)"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "val churn_predictions_test = learned_model.transform(test_churn)\nevaluator.evaluate(churn_predictions_test)", "execution_count": 16, "outputs": [{"output_type": "execute_result", "execution_count": 16, "data": {"text/plain": "0.9107948973116434"}, "metadata": {}}]}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "churn_predictions_test.select(\"phone_number\", \"churned\", \"churn_label\", \"prediction\", \"predictedLabel\", \"probability\", \"rawPrediction\").show()", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "+------------+-------+-----------+----------+--------------+--------------------+--------------------+\n|phone_number|churned|churn_label|prediction|predictedLabel|         probability|       rawPrediction|\n+------------+-------+-----------+----------+--------------+--------------------+--------------------+\n|    375-4450| False.|        0.0|       0.0|        False.|[0.96315495912603...|[19.2630991825207...|\n|    414-7942| False.|        0.0|       0.0|        False.|[0.96240331885511...|[19.2480663771022...|\n|    390-2346| False.|        0.0|       0.0|        False.|[0.94944124734433...|[18.9888249468866...|\n|    336-6533| False.|        0.0|       0.0|        False.|[0.83655599205242...|[16.7311198410484...|\n|    404-1931| False.|        0.0|       0.0|        False.|[0.95242437278993...|[19.0484874557986...|\n|    389-4602| False.|        0.0|       0.0|        False.|[0.80754245100406...|[16.1508490200813...|\n|    334-4506| False.|        0.0|       0.0|        False.|[0.96277113366150...|[19.2554226732301...|\n|    394-5202| False.|        0.0|       0.0|        False.|[0.96185869884476...|[19.2371739768952...|\n|    362-7200| False.|        0.0|       0.0|        False.|[0.96076517735521...|[19.2153035471043...|\n|    372-8261| False.|        0.0|       0.0|        False.|[0.95800230109628...|[19.1600460219257...|\n|    396-2335| False.|        0.0|       0.0|        False.|[0.95409161290265...|[19.0818322580530...|\n|    394-4548| False.|        0.0|       0.0|        False.|[0.95456121573704...|[19.0912243147409...|\n|    399-8009| False.|        0.0|       0.0|        False.|[0.95246981982128...|[19.0493963964257...|\n|    363-1245| False.|        0.0|       0.0|        False.|[0.93785735174793...|[18.7571470349587...|\n|    366-6991| False.|        0.0|       0.0|        False.|[0.96320633434037...|[19.2641266868074...|\n|    378-5140| False.|        0.0|       0.0|        False.|[0.95173785254508...|[19.0347570509016...|\n|    345-8237| False.|        0.0|       0.0|        False.|[0.96352812914621...|[19.2705625829242...|\n|    402-1381| False.|        0.0|       0.0|        False.|[0.85390677926300...|[17.0781355852600...|\n|    404-9680| False.|        0.0|       0.0|        False.|[0.94027806628583...|[18.8055613257166...|\n|    375-2975|  True.|        1.0|       1.0|         True.|[0.33630169745066...|[6.72603394901334...|\n+------------+-------+-----------+----------+--------------+--------------------+--------------------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Perf. Confusion Matrix"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "val predictionsAndLabels = churn_predictions_test.\n                            select(\"churn_label\", \"prediction\").\n                            rdd.map { e => (e.getDouble(0), e.getDouble(1))}\n\nval metrics = new MulticlassMetrics(predictionsAndLabels)\n\nval confusionMatrix = metrics.confusionMatrix\n\nconfusionMatrix", "execution_count": 18, "outputs": [{"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "1285.0  99.0\n11.0    115.0"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Perf. ROC"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "val rawPredictionAndLabels = churn_predictions_test.\n                            select(\"prediction\", \"probability\").\n                            rdd.map { e => \n                                val (pred, Array(prob_class0, prob_class1)) = (e.getDouble(0), e(1).asInstanceOf[MLDenseVector].toArray)\n                                (prob_class1, pred)\n                            }\n\nval metrics = new BinaryClassificationMetrics(rawPredictionAndLabels)\n\nval roc = metrics.roc.collect", "execution_count": 19, "outputs": []}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "roc.take(20).foreach(println)", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "(0.0,0.0)\n(0.0,0.007936507936507936)\n(0.0,0.015873015873015872)\n(0.0,0.023809523809523808)\n(0.0,0.031746031746031744)\n(0.0,0.03968253968253968)\n(0.0,0.047619047619047616)\n(0.0,0.05555555555555555)\n(0.0,0.06349206349206349)\n(0.0,0.07142857142857142)\n(0.0,0.07936507936507936)\n(0.0,0.0873015873015873)\n(0.0,0.09523809523809523)\n(0.0,0.10317460317460317)\n(0.0,0.1111111111111111)\n(0.0,0.11904761904761904)\n(0.0,0.12698412698412698)\n(0.0,0.1349206349206349)\n(0.0,0.14285714285714285)\n(0.0,0.15079365079365079)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Perf. Lift Curve"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "val nbPoints = 100\n\nval predProbs = churn_predictions_test.select(\"churn_label\", \"probability\").\n                    rdd.map { e => \n                            val (pred, Array(prob_class0, prob_class1)) = (e(0), e(1).asInstanceOf[MLDenseVector].toArray)\n                            (pred, prob_class1)\n                    }.\n                    collect.\n                    sortBy(-_._2)\n\nval sampleSize = predProbs.size\n\nval evalMetrics = (1 to nbPoints).reverse.map { e =>\n                                       \n    val threshold: Double = e.toDouble / nbPoints\n                                                                              \n    // Sensitivity or Recall\n    val sensDF = predProbs.filter(f => f._2 > threshold)\n    val nSens = sensDF.size\n    val tp = sensDF.map(_._1).filter(_ == 1.0).size // true pos\n    val fn = sensDF.map(_._1).filter(_ == 0.0).size // false neg\n    val sensitivity: Double = if(nSens > 0) tp.toDouble / nSens else 0.0\n                                        \n    // Specificity computation\n    //val specDF = predProbs.filter(f => f._2 <= threshold)\n    //val tn = specDF.map(_._1).filter(_ == 0.0).size // true neg\n    //val fp = specDF.map(_._1).filter(_ == 1.0).size // false pos\n    //val nSpec  = specDF.size\n    //val specificity: Double = if(nSpec > 0) tn.toDouble / nSpec else 0.0\n    \n    // Precision\n    //val precision = tp.toDouble / (tp + fp) \n\n    // Support\n    val supp = nSens.toDouble / sampleSize\n    //(supp, sensitivity) // GAIN CURVE                                                 \n    (supp, sensitivity / supp) // LIFT CURVE\n    // (1-specificity, sensitivity) // ROC \n}.\ndistinct.\nfilter(_._2 > 0)", "execution_count": 21, "outputs": []}, {"metadata": {"scrolled": false, "collapsed": false}, "cell_type": "code", "source": "evalMetrics.take(20).foreach(println)", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "(0.0013245033112582781,755.0)\n(0.001986754966887417,503.3333333333333)\n(0.0033112582781456954,302.0)\n(0.003973509933774834,251.66666666666666)\n(0.008609271523178808,116.15384615384616)\n(0.011920529801324504,83.88888888888889)\n(0.015231788079470199,65.65217391304347)\n(0.016556291390728478,60.4)\n(0.017880794701986755,55.925925925925924)\n(0.02052980132450331,47.138397502601464)\n(0.023178807947019868,41.91020408163265)\n(0.025827814569536423,37.72518080210388)\n(0.02781456953642384,35.09637188208617)\n(0.029801324503311258,32.809876543209874)\n(0.03377483443708609,29.027297193387156)\n(0.03642384105960265,26.955371900826446)\n(0.0423841059602649,23.22509765625)\n(0.046357615894039736,21.26326530612245)\n(0.04966887417218543,19.864888888888892)\n(0.051655629139072845,19.110782380013152)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Saving the model"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "val model_path = \"swift://DubaiH2OMeetup.\" + name + \"/rf_churn.model_v0\"\nsc.parallelize(Seq(learned_model), 1).saveAsObjectFile(model_path)", "execution_count": 26, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Reload Model"}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "import org.apache.spark.ml.PipelineModel\nval loaded_model = sc.objectFile[PipelineModel](model_path).first", "execution_count": 27, "outputs": []}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "val churn_predictions_test_fromLoadedModel = loaded_model.transform(test_churn)\nevaluator.evaluate(churn_predictions_test_fromLoadedModel)", "execution_count": 28, "outputs": [{"output_type": "execute_result", "execution_count": 28, "data": {"text/plain": "0.9107948973116434"}, "metadata": {}}]}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "churn_predictions_test_fromLoadedModel.select(\"phone_number\", \"churned\", \"churn_label\", \"prediction\", \"probability\").show()", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "+------------+-------+-----------+----------+--------------------+\n|phone_number|churned|churn_label|prediction|         probability|\n+------------+-------+-----------+----------+--------------------+\n|    375-4450| False.|        0.0|       0.0|[0.96315495912603...|\n|    414-7942| False.|        0.0|       0.0|[0.96240331885511...|\n|    390-2346| False.|        0.0|       0.0|[0.94944124734433...|\n|    336-6533| False.|        0.0|       0.0|[0.83655599205242...|\n|    404-1931| False.|        0.0|       0.0|[0.95242437278993...|\n|    389-4602| False.|        0.0|       0.0|[0.80754245100406...|\n|    334-4506| False.|        0.0|       0.0|[0.96277113366150...|\n|    394-5202| False.|        0.0|       0.0|[0.96185869884476...|\n|    362-7200| False.|        0.0|       0.0|[0.96076517735521...|\n|    372-8261| False.|        0.0|       0.0|[0.95800230109628...|\n|    396-2335| False.|        0.0|       0.0|[0.95409161290265...|\n|    394-4548| False.|        0.0|       0.0|[0.95456121573704...|\n|    399-8009| False.|        0.0|       0.0|[0.95246981982128...|\n|    363-1245| False.|        0.0|       0.0|[0.93785735174793...|\n|    366-6991| False.|        0.0|       0.0|[0.96320633434037...|\n|    378-5140| False.|        0.0|       0.0|[0.95173785254508...|\n|    345-8237| False.|        0.0|       0.0|[0.96352812914621...|\n|    402-1381| False.|        0.0|       0.0|[0.85390677926300...|\n|    404-9680| False.|        0.0|       0.0|[0.94027806628583...|\n|    375-2975|  True.|        1.0|       1.0|[0.33630169745066...|\n+------------+-------+-----------+----------+--------------------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "nbformat": 4, "nbformat_minor": 0}